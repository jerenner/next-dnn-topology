{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs. background classification with NEW full MC\n",
    "\n",
    "In this notebook we read in the prepared data, construct and train the DNN, and then evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrenner/anaconda3/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/Users/jrenner/anaconda3/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import random as rd\n",
    "import tables as tb\n",
    "import h5py\n",
    "\n",
    "from matplotlib.patches         import Ellipse\n",
    "from __future__  import print_function\n",
    "from scipy.stats import threshold\n",
    "\n",
    "# Keras imports\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models               import Model, load_model\n",
    "from keras.layers               import Input, Dense, MaxPooling3D, AveragePooling3D, Convolution3D, Activation, Dropout, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers           import SGD, Adam, Nadam         \n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core          import Flatten\n",
    "from keras                      import callbacks\n",
    "from keras.regularizers         import l2, activity_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the variables for reading in the data\n",
    "data_location = \"/Users/jrenner/IFIC/jerenner/next-dnn-topology/data\"\n",
    "run_name = \"1M_v0_08_07\"\n",
    "\n",
    "# set the data dimensions\n",
    "xdim = 20\n",
    "ydim = 20\n",
    "zdim = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the function to read the data from multiple files\n",
    "def read_data(loc, rname, f_start, f_end):\n",
    "    \"\"\"Reads all events from the files with the specified file numbers.\"\"\"\n",
    "    \n",
    "    # read in the signal events.\n",
    "    print(\"Reading signal events...\")\n",
    "    for fn in range(f_start,f_end):\n",
    "        s_dat = tb.open_file(\"{0}/{1}/hdf5_maps_NEW_training_MC_si_{2}.h5\".format(loc,rname,fn), 'r')\n",
    "        if(fn == f_start):\n",
    "            s_array = np.array(s_dat.root.maps)\n",
    "            s_energies = np.array(s_dat.root.energies)\n",
    "            print(\"-- Reading file {0},\".format(fn), end=' ')\n",
    "        else:\n",
    "            print(\"{0},\".format(fn), end=' ')\n",
    "            s_array = np.concatenate([s_array,np.array(s_dat.root.maps)])\n",
    "            s_energies = np.concatenate([s_energies,np.array(s_dat.root.energies)])\n",
    "\n",
    "    # read in the background events.\n",
    "    print(\"\\nReading background events...\")\n",
    "    for fn in range(f_start,f_end):\n",
    "        b_dat = tb.open_file(\"{0}/{1}/hdf5_maps_NEW_training_MC_bg_{2}.h5\".format(loc,rname,fn), 'r')\n",
    "        if(fn == f_start):\n",
    "            print(\"-- Reading file {0},\".format(fn), end=' ')\n",
    "            b_array = np.array(b_dat.root.maps)\n",
    "            b_energies = np.array(b_dat.root.energies)\n",
    "        else:\n",
    "            print(\"{0},\".format(fn), end=' ')\n",
    "            b_array = np.concatenate([b_array,np.array(b_dat.root.maps)])\n",
    "            b_energies = np.concatenate([b_energies,np.array(b_dat.root.energies)])\n",
    "    print(\"\\nRead {0} signal events and {1} background events.\".format(len(s_array),len(b_array)))\n",
    "        \n",
    "    # concatenate the datasets\n",
    "    print(\"Concatenating datasets...\")\n",
    "    x_ = np.concatenate([s_array, b_array])\n",
    "    y_ = np.concatenate([np.ones([len(s_array), 1]), np.zeros([len(b_array), 1])])\n",
    "    \n",
    "    # reshape for training with TensorFlow\n",
    "    x_ = np.reshape(x_, (len(x_), xdim, ydim, zdim, 1))\n",
    "    \n",
    "    return x_,y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading signal events...\n",
      "-- Reading file 0 -- Reading file 1 Reading background events...\n",
      "-- Reading file 0 -- Reading file 1 Read 3813 signal events and 3601 background events.\n",
      "Concatenating datasets...\n"
     ]
    }
   ],
   "source": [
    "# read in the training data\n",
    "x_train, y_train = read_data(data_location, run_name, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a 3D convolutional neural network\n",
    "def model_3Dconv():\n",
    "    \n",
    "    cinputs = Convolution3D(256, 5, 5, 5, border_mode='same', subsample=(4, 4, 4), activation='relu',init='lecun_uniform', W_regularizer=l2(0.000001))(inputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = Convolution3D(128, 2, 2, 3, border_mode='same', subsample=(2, 2, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = Convolution3D(128, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.000001))(cinputs)\n",
    "    cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "    f1 = Flatten()(cinputs)\n",
    "    f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.000001))(f1)\n",
    "    f1 = Dropout(.3)(f1)\n",
    "\n",
    "    inc_output = Dense(output_dim=1, activation='sigmoid',init='normal', W_regularizer=l2(0.000001))(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Nadam(lr=0.00001, beta_1=0.9, beta_2=0.999,\n",
    "                                  epsilon=1e-08, schedule_decay=0.01), metrics=['accuracy'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 20, 20, 60, 1) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_5 (Convolution3D)  (None, 5, 5, 15, 256) 32256       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_3 (MaxPooling3D)    (None, 3, 3, 8, 256)  0           convolution3d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 3, 3, 8, 256)  512         maxpooling3d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_6 (Convolution3D)  (None, 3, 3, 8, 64)   16448       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_7 (Convolution3D)  (None, 2, 2, 3, 128)  98432       convolution3d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 2, 2, 3, 128)  256         convolution3d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_8 (Convolution3D)  (None, 2, 2, 3, 128)  16512       batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_4 (MaxPooling3D)    (None, 1, 1, 2, 128)  0           convolution3d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 256)           0           maxpooling3d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           32896       flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 128)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             129         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 197441\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# set load_model to true and specify the file to load in a previously defined/trained model\n",
    "load_model = False\n",
    "mfile = 'models/conv3d_classifier.h5'\n",
    "\n",
    "if(load_model):\n",
    "    incep = load_model(mfile)\n",
    "else:\n",
    "    \n",
    "    # otherwise define the model\n",
    "    model = model_3Dconv()\n",
    "    \n",
    "    # define callbacks (actions to be taken after each epoch of training)\n",
    "    lcallbacks = [callbacks.ModelCheckpoint('models/conv3d_classifier_20.h5', monitor='val_loss', save_best_only=True, mode='min')]            \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 8000 samples\n",
      "Epoch 1/60\n",
      "80000/80000 [==============================] - 29s - loss: 0.6405 - acc: 0.6391 - val_loss: 0.7054 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "80000/80000 [==============================] - 27s - loss: 0.5524 - acc: 0.7325 - val_loss: 0.5340 - val_acc: 0.7450\n",
      "Epoch 3/60\n",
      "79900/80000 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.7524"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "hist = model.fit(x_t, y_t, shuffle=True, nb_epoch=60, batch_size=100, verbose=1, validation_data=(x_v, y_v), callbacks=lcallbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TEST EVENTS\n",
    "f_start = 0\n",
    "f_end = 25\n",
    "# Read in the signal events.\n",
    "print(\"Reading test signal events...\")\n",
    "for fn in range(f_start,f_end):\n",
    "    print(\"-- Reading file {0}\".format(fn))\n",
    "    s_dat = tb.open_file(\"/home/jrenner/data/classification/bb_1M_v0_08_07/hdf5_maps_NEW_training_MC_si_{0}.h5\".format(fn), 'r')\n",
    "    if(fn == f_start):\n",
    "        stest_array = np.array(s_dat.root.maps)\n",
    "        stest_energies = np.array(s_dat.root.energies)\n",
    "    else:\n",
    "        stest_array = np.concatenate([stest_array,np.array(s_dat.root.maps)])\n",
    "        stest_energies = np.concatenate([stest_energies,np.array(s_dat.root.energies)])\n",
    "        \n",
    "# Read in the background events.\n",
    "print(\"Reading background events...\")\n",
    "for fn in range(f_start,f_end):\n",
    "    print(\"-- Reading file {0}\".format(fn))\n",
    "    b_dat = tb.open_file(\"/home/jrenner/data/classification/se_1M_v0_08_07/hdf5_maps_NEW_training_MC_bg_{0}.h5\".format(fn), 'r')\n",
    "    if(fn == f_start):\n",
    "        btest_array = np.array(b_dat.root.maps)\n",
    "        btest_energies = np.array(b_dat.root.energies)\n",
    "    else:\n",
    "        btest_array = np.concatenate([btest_array,np.array(b_dat.root.maps)])\n",
    "        btest_energies = np.concatenate([btest_energies,np.array(b_dat.root.energies)])\n",
    "\n",
    "print(\"Read {0} test signal events and {1} test background events.\".format(len(stest_array),len(btest_array)))\n",
    "\n",
    "# Concatenate the datasets\n",
    "print(\"Concatenating datasets...\")\n",
    "x_e = np.concatenate([stest_array, btest_array])\n",
    "y_e = np.concatenate([np.ones([len(stest_array),1]), np.zeros([len(btest_array),1])])\n",
    "x_e = np.reshape(x_e, (len(x_e), xdim, ydim, zdim, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_and_metrics = incep.evaluate(x_e, y_e);\n",
    "y_pred = incep.predict(x_e, batch_size=100, verbose=0)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for thh in [0.897818]:#np.arange(0,1,0.01):\n",
    "    nts = 0; ntb = 0\n",
    "    ncs = 0; ncb = 0\n",
    "    for ye,yp in zip(y_e,y_pred):\n",
    "        if(ye == 0):\n",
    "            ntb += 1  # add one background event\n",
    "            if(yp < thh):\n",
    "                ncb += 1  # add one correctly predicted background event\n",
    "\n",
    "        if(ye == 1):\n",
    "            nts += 1  # add one signal event\n",
    "            if(yp >= thh):\n",
    "                ncs += 1  # add one correctly predicted signal event\n",
    "\n",
    "    print(\"-- {0} of {1} ({2}%) correct background events; {3} of {4} ({5}%) correct signal events\".format(ncb,ntb,1.0*ncb/ntb*100,ncs,nts,1.0*ncs/nts*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incep.save('models/largenet_noise.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- With of order 11k params, reached val loss of about 0.52 and seemed to slow significantly\n",
    "- With of order 30k params, reached val loss of about 0.49 at best\n",
    "- The problem of fluctuating validation loss was due to a learning rate that was too high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of 20x20 window table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate table consisting of 20x20 windows for each SiPM with maximum charge\n",
    "# -- Table is 2304x400: for (i,j) in (0,0) to (48,48); the table contains the list \n",
    "#     of IDs for the corresponding 20x20 window for each 1D SiPM ID = i*48 + j\n",
    "\n",
    "# Create the HDF5 file.\n",
    "h5f = h5py.File(\"wtbl.h5\")\n",
    "\n",
    "tbl = np.zeros([48*48,400])\n",
    "print(\"Generating window table...\")\n",
    "for i in range(48):\n",
    "    for j in range(48):\n",
    "        sipm_id = i*48 + j\n",
    "        \n",
    "        # Determine the 20x20 window.\n",
    "        i_in = i - 10; i_fi = i + 10\n",
    "        if(i_in < 0):\n",
    "            i_fi = (i - i_in) + 10\n",
    "            i_in = 0\n",
    "        elif(i_fi > 48):\n",
    "            i_in = i - (i_fi - 48) - 10\n",
    "            i_fi = 48\n",
    "        j_in = j - 10; j_fi = j + 10\n",
    "        if(j_in < 0):\n",
    "            j_fi = (j - j_in) + 10\n",
    "            j_in = 0\n",
    "        elif(j_fi > 48):\n",
    "            j_in = j - (j_fi - 48) - 10\n",
    "            j_fi = 48\n",
    "            \n",
    "        # Save the 20x20 window in the table.\n",
    "        nwin = 0\n",
    "        for iw in range(i_in,i_fi):\n",
    "            for jw in range(j_in,j_fi):\n",
    "                w_id = iw*48 + jw\n",
    "                tbl[sipm_id][nwin] = w_id\n",
    "                nwin += 1\n",
    "\n",
    "# Save the table to an HDF5 file.\n",
    "print(\"Saving table to file...\")\n",
    "h5f.create_dataset(\"wtbl\",data=tbl)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20x20 window plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carried over from NEW_kr_diff_mc_train.ipynb\n",
    "def NEW_SiPM_map_plot(xarr, yarr, plot_truth=True, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots a SiPM map in the NEW Geometry\n",
    "    xarr is a NEW sipm map, yarr the pair of coordinates the map corresponds to\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        probs = (xarr - np.min(xarr))\n",
    "        probs /= np.max(probs)\n",
    "    else: \n",
    "        probs = xarr\n",
    "\n",
    "    fig = plt.figure();\n",
    "    ax1 = fig.add_subplot(111);\n",
    "    fig.set_figheight(5.0)\n",
    "    fig.set_figwidth(5.0)\n",
    "    ax1.axis([-100, 100, -100, 100]);\n",
    "\n",
    "    for i in range(20):\n",
    "        for j in range(20):\n",
    "            r = Ellipse(xy=(i * 10 - 100, j * 10 - 100), width=4., height=4.);\n",
    "            r.set_facecolor('0');\n",
    "            r.set_alpha(probs[i, j]);\n",
    "            ax1.add_artist(r);\n",
    "            \n",
    "    if plot_truth:\n",
    "        # Place a large blue circle for actual EL points.\n",
    "        xpt = yarr[0]\n",
    "        ypt = yarr[1]\n",
    "        mrk = Ellipse(xy=(xpt,ypt), width=4., height=4.);\n",
    "        mrk.set_facecolor('b');\n",
    "        ax1.add_artist(mrk);\n",
    "        #print(xpt,ypt)\n",
    "        \n",
    "    plt.xlabel(\"x (mm)\");\n",
    "    plt.ylabel(\"y (mm)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot training event slices.\n",
    "plt_nevt = 0\n",
    "plt_nslice = 6\n",
    "\n",
    "plt_arr = x_t[plt_nevt,:,:,plt_nslice,0]\n",
    "NEW_SiPM_map_plot(plt_arr,[0, 0], False)\n",
    "chg_sum = np.sum(plt_arr)\n",
    "tot_chg_sum = np.sum(x_t[plt_nevt,:,:,:])\n",
    "max_chg = np.max(x_t[plt_nevt,:,:,plt_nslice,0])\n",
    "min_chg = np.min(x_t[plt_nevt,:,:,plt_nslice,0])\n",
    "print(\"Plotting event\", plt_nevt, \"slice\", plt_nslice, \"with charge sum\", chg_sum, \"and total sum\", tot_chg_sum,\n",
    "     \"max charge\", max_chg, \"and min charge\", min_chg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the energies of the training events.\n",
    "esums = []\n",
    "for earr in s_earray:\n",
    "    esums.append(np.sum(earr)/12.)\n",
    "#for earr in b_earray:\n",
    "#    esums.append(np.sum(earr))\n",
    "    \n",
    "fig = plt.figure();\n",
    "ax1 = fig.add_subplot(111);\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(7.5)\n",
    "\n",
    "plt.hist(esums, 100, normed=1, facecolor='green')\n",
    "plt.xlabel('Cathode charge')\n",
    "plt.ylabel('Counts/bin')\n",
    "plt.show()\n",
    "print(b_earray[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carried over from NEW_kr_diff_mc_train.ipynb\n",
    "def NEW_SiPM_map_plot(xarr, yarr, plot_truth=True, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots a SiPM map in the NEW Geometry\n",
    "    xarr is a NEW sipm map, yarr the pair of coordinates the map corresponds to\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        probs = (xarr - np.min(xarr))\n",
    "        probs /= np.max(probs)\n",
    "    else: \n",
    "        probs = xarr\n",
    "\n",
    "    fig = plt.figure();\n",
    "    ax1 = fig.add_subplot(111);\n",
    "    fig.set_figheight(7.0)\n",
    "    fig.set_figwidth(7.0)\n",
    "    ax1.axis([-250, 250, -250, 250]);\n",
    "\n",
    "    for i in range(48):\n",
    "        for j in range(48):\n",
    "            r = Ellipse(xy=(i * 10 - 235, j * 10 - 235), width=2., height=2.);\n",
    "            r.set_facecolor('0');\n",
    "            r.set_alpha(probs[i, j]);\n",
    "            ax1.add_artist(r);\n",
    "            \n",
    "    if plot_truth:\n",
    "        # Place a large blue circle for actual EL points.\n",
    "        xpt = yarr[0]\n",
    "        ypt = yarr[1]\n",
    "        mrk = Ellipse(xy=(xpt,ypt), width=4., height=4.);\n",
    "        mrk.set_facecolor('b');\n",
    "        ax1.add_artist(mrk);\n",
    "        #print(xpt,ypt)\n",
    "        \n",
    "    plt.xlabel(\"x (mm)\");\n",
    "    plt.ylabel(\"y (mm)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot training event slices.\n",
    "plt_nevt = 14900\n",
    "plt_nslice = 5\n",
    "\n",
    "plt_arr = x_t[plt_nevt,:,:,plt_nslice]\n",
    "NEW_SiPM_map_plot(plt_arr,[0, 0], False)\n",
    "chg_sum = np.sum(plt_arr)\n",
    "tot_chg_sum = np.sum(x_t[plt_nevt,:,:,:])\n",
    "max_chg = np.max(x_t[plt_nevt,:,:,plt_nslice])\n",
    "min_chg = np.min(x_t[plt_nevt,:,:,plt_nslice])\n",
    "print(\"Plotting event\", plt_nevt, \"slice\", plt_nslice, \"with charge sum\", chg_sum, \"and total sum\", tot_chg_sum,\n",
    "     \"max charge\", max_chg, \"and min charge\", min_chg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a histogram containing the SiPM distribution for a given (x,y,z), where x,y,z are indices in the map.\n",
    "def SiPM_dist(sipm_maps,x,y,z):\n",
    "    \n",
    "    return sipm_maps[:,x,y,z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine which SiPMs have all-zero distributions for a given slice.\n",
    "slnum = 0\n",
    "for sipmx in range(xdim):\n",
    "    for sipmy in range(ydim):\n",
    "        \n",
    "        dist = x_t[:,sipmx,sipmy,slnum]\n",
    "        nzeros = len(np.nonzero(dist)[0])\n",
    "        if(nzeros == 0):\n",
    "            print(\"All zeros for SiPM ({0},{1})\".format(sipmx,sipmy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 12 distributions.\n",
    "fig = plt.figure();\n",
    "fig.set_figheight(20.0)\n",
    "fig.set_figwidth(13.0)\n",
    "\n",
    "for ndist in range(3):\n",
    "    sp = int(430 + ndist + 1)\n",
    "    print(\"plot\",sp)\n",
    "    ax = fig.add_subplot(sp);\n",
    "    \n",
    "    xv = np.random.randint(38) + 5\n",
    "    yv = np.random.randint(38) + 5 \n",
    "    sv = np.random.randint(14)\n",
    "\n",
    "    dist = x_t[:,xv,yv,sv]\n",
    "    plt.hist(dist[np.nonzero(dist)], 1000, normed=0, facecolor='green')\n",
    "    ax.set_yscale('log')\n",
    "    start, end = ax.get_xlim()\n",
    "    #ax.xaxis.set_ticks(np.arange(start, end, (end-start)/4.))\n",
    "    plt.xlim(0.0,0.05)\n",
    "    plt.xlabel('SiPM Charge')\n",
    "    plt.ylabel('Counts/bin')\n",
    "    plt.title('SiPM ({0},{1}); slice {2}'.format(xv,yv,sv))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot SiPM sum distributions for a range of events.\n",
    "#tr_evt_no = 15\n",
    "fig = plt.figure();\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(5.0)\n",
    "\n",
    "s_values = []\n",
    "for tr_evt_no in range(1000):\n",
    "    for nx in range(xdim):\n",
    "        for ny in range(ydim):\n",
    "            sval = np.sum(x_t[tr_evt_no,nx,ny,:])\n",
    "            if(sval != 0):\n",
    "                s_values.append(sval)\n",
    "        #print(\"SiPM at ({0},{1})\".format(nx,ny))\n",
    "\n",
    "ax = fig.add_subplot(111);\n",
    "plt.hist(s_values, 100, normed=0, facecolor='green')\n",
    "ax.set_yscale('log')\n",
    "#start, end = ax.get_xlim()\n",
    "#ax.xaxis.set_ticks(np.arange(start, end, (end-start)/4.))\n",
    "plt.xlim(0.0,0.3)\n",
    "plt.xlabel('SiPM Charge Sum')\n",
    "plt.ylabel('Counts/bin')\n",
    "#plt.title('Event {0}'.format(tr_evt_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old nets\n",
    "        cinputs = Convolution3D(32, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(32, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        \n",
    "        \n",
    "\n",
    "        inputs = Input(shape=(48, 48, 30, 1))\n",
    "        cinputs = Convolution3D(512, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(1024, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=1024, activation='relu', init='normal', W_regularizer=l2(0.001))(f1)\n",
    "        f1 = Dropout(.4)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal')(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        cinputs = Convolution3D(32, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='sigmoid',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        #cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='tanh',init='normal')(cinputs)\n",
    "        cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='sigmoid',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        #cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='tanh',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "\n",
    "        inputs = Input(shape=(48, 48, 30))\n",
    "        cinputs = Convolution2D(512, 6, 6, border_mode='same', subsample=(3, 3), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution2D(64, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution2D(1024, 2, 2, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution2D(16, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        \n",
    "        # 2D net 16-12-16\n",
    "        inputs = Input(shape=(48, 48, 30))\n",
    "        cinputs = Convolution2D(32, 6, 6, border_mode='same', subsample=(3, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(inputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        #cinputs = Convolution2D(8, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.05))(cinputs)\n",
    "        cinputs = Convolution2D(64, 2, 2, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution2D(128, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.008))(f1)\n",
    "        f1 = Dropout(.6)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='lecun_uniform', W_regularizer=l2(0.001))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "        \n",
    "        # Good 3D net 16-12-16\n",
    "        inputs = Input(shape=(48, 48, 30, 1))\n",
    "        cinputs = Convolution3D(128, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = Convolution3D(128, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=64, activation='relu', init='lecun_uniform', W_regularizer=l2(0.02))(f1)\n",
    "        f1 = Dropout(.7)(f1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # DNN for 20x20x30\n",
    "        inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "        cinputs = Convolution3D(256, 5, 5, 5, border_mode='same', subsample=(4, 4, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(128, 2, 2, 3, border_mode='same', subsample=(2, 2, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(128, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.001))(f1)\n",
    "        f1 = Dropout(.7)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal', W_regularizer=l2(0.001))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "\n",
    "        incep.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.00001, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.1), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add noise\n",
    "nsigma = 1.0e-4\n",
    "\n",
    "inoise = x_t == 0.\n",
    "x_t[inoise] = np.abs(np.random.normal(0,1.0e-4,np.sum(inoise)))\n",
    "\n",
    "inoise = x_v == 0.\n",
    "x_v[inoise] = np.abs(np.random.normal(0,1.0e-4,np.sum(inoise)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OLD READ METHOD\n",
    "\n",
    "# Signal events.\n",
    "s_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_si_20.h5', 'r')\n",
    "#s_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_si_nst_nonorm.h5', 'r')\n",
    "print(s_dat)\n",
    "s_array = np.array(s_dat.root.maps)\n",
    "x_t = s_array[:Ntrain]\n",
    "x_v = s_array[Ntrain:Ntot-Ntest]\n",
    "x_e = s_array[Ntot-Ntest:Ntot]\n",
    "y_t = np.ones([Ntrain, 1])\n",
    "y_v = np.ones([Ntot-Ntrain-Ntest, 1])\n",
    "y_e = np.ones([Ntest, 1])\n",
    "\n",
    "s_earray = np.array(s_dat.root.energies)\n",
    "\n",
    "# Background events.\n",
    "b_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_bg_20.h5', 'r')\n",
    "#b_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_bg_nst_nonorm.h5', 'r')\n",
    "print(b_dat)\n",
    "b_array = np.array(b_dat.root.maps)\n",
    "print(\"Concatenating datasets...\")\n",
    "x_t = np.concatenate([x_t, b_array[:Ntrain]])\n",
    "x_v = np.concatenate([x_v, b_array[Ntrain:Ntot-Ntest]])\n",
    "x_e = np.concatenate([x_e, b_array[Ntot-Ntest:Ntot]])\n",
    "y_bt = np.zeros([Ntrain, 1])\n",
    "y_t = np.concatenate([y_t, y_bt])\n",
    "y_bv = np.zeros([Ntot-Ntrain-Ntest, 1])\n",
    "y_v = np.concatenate([y_v, y_bv])\n",
    "y_be = np.zeros([Ntest, 1])\n",
    "y_e = np.concatenate([y_e, y_be])\n",
    "\n",
    "b_earray = np.array(b_dat.root.energies)\n",
    "\n",
    "# Normalize\n",
    "#mval = max(np.max(s_array),np.max(b_array))\n",
    "#muval = np.mean(s_array)\n",
    "#sval = np.std(s_array)\n",
    "#print(\"Normalizing with max value of\", mval, \"(mean of\", muval, \"; sigma of \", sval, \")\")\n",
    "#x_t /= sval\n",
    "#x_v /= sval"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
