{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Signal vs. background classification with NEW full MC\n",
    "\n",
    "In this notebook we read in the prepared data, construct and train the DNN, and then evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import random as rd\n",
    "import tables as tb\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.patches         import Ellipse\n",
    "from __future__  import print_function\n",
    "from scipy.stats import threshold\n",
    "\n",
    "# Keras imports\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models               import Model, load_model\n",
    "from keras.layers               import Input, Dense, MaxPooling3D, AveragePooling3D, Convolution3D, Activation, Dropout, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers           import SGD, Adam, Nadam         \n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core          import Flatten\n",
    "from keras                      import callbacks\n",
    "from keras.regularizers         import l2, activity_l2\n",
    "tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Variable definitions\n",
    "Here we define key variables to be used throughout the notebook.  Note that we will read the data from a directory `data_location/run_name`, and it is stored in multiple files:\n",
    "- The training data will consist of the events stored in files from `train_fstart` to `train_fend`\n",
    "- The test data will consist of the events stored in files from `test_fstart` to `test_fend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "xdim = 20\n",
    "ydim = 20\n",
    "zdim = 60\n",
    "\n",
    "# data location and training/test file numbers\n",
    "data_location = \"/home/jrenner/data/classification\"\n",
    "run_name = \"1M_v0_08_07\"\n",
    "train_fstart = 0\n",
    "train_fend = 2\n",
    "test_fstart = 4\n",
    "test_fend = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Function definitions\n",
    "\n",
    "### Data input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the function to read the data from multiple files\n",
    "def read_data(loc, rname, f_start, f_end):\n",
    "    \"\"\"Reads all events from the files with the specified file numbers.\"\"\"\n",
    "    \n",
    "    # read in the signal events.\n",
    "    print(\"Reading signal events...\")\n",
    "    for fn in range(f_start,f_end):\n",
    "        s_dat = tb.open_file(\"{0}/{1}/hdf5_maps_NEW_training_MC_si_{2}.h5\".format(loc,rname,fn), 'r')\n",
    "        if(fn == f_start):\n",
    "            s_array = np.array(s_dat.root.maps)\n",
    "            s_energies = np.array(s_dat.root.energies)\n",
    "            print(\"-- Reading file {0},\".format(fn), end=' ')\n",
    "        else:\n",
    "            print(\"{0},\".format(fn), end=' ')\n",
    "            s_array = np.concatenate([s_array,np.array(s_dat.root.maps)])\n",
    "            s_energies = np.concatenate([s_energies,np.array(s_dat.root.energies)])\n",
    "    print(\"done.\")\n",
    "\n",
    "    # read in the background events.\n",
    "    print(\"\\nReading background events...\")\n",
    "    for fn in range(f_start,f_end):\n",
    "        b_dat = tb.open_file(\"{0}/{1}/hdf5_maps_NEW_training_MC_bg_{2}.h5\".format(loc,rname,fn), 'r')\n",
    "        if(fn == f_start):\n",
    "            print(\"-- Reading file {0},\".format(fn), end=' ')\n",
    "            b_array = np.array(b_dat.root.maps)\n",
    "            b_energies = np.array(b_dat.root.energies)\n",
    "        else:\n",
    "            print(\"{0},\".format(fn), end=' ')\n",
    "            b_array = np.concatenate([b_array,np.array(b_dat.root.maps)])\n",
    "            b_energies = np.concatenate([b_energies,np.array(b_dat.root.energies)])\n",
    "    print(\"done.\")\n",
    "    print(\"\\nRead {0} signal events and {1} background events.\".format(len(s_array),len(b_array)))\n",
    "        \n",
    "    # concatenate the datasets\n",
    "    print(\"Concatenating datasets...\", end=' ')\n",
    "    x_ = np.concatenate([s_array, b_array])\n",
    "    y_ = np.concatenate([np.ones([len(s_array), 1]), np.zeros([len(b_array), 1])])\n",
    "    print(\"done.\")\n",
    "    \n",
    "    # reshape for training with TensorFlow\n",
    "    print(\"Reshaping projection...\", end=' ')\n",
    "    x_ = np.reshape(x_, (len(x_), xdim, ydim, zdim, 1))\n",
    "    print(\"done.\")\n",
    "    \n",
    "    print(\"Finished reading data.\")\n",
    "    return x_,y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural network models\n",
    "These functions should define and return a Keras model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Define more neural networks here\n",
    "\n",
    "def model_(inputs):\n",
    "    \n",
    "    # define the network\n",
    "    \n",
    "    # ----------\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a fully-connected neural network with 64 hidden neurons and 1 readout neuron\n",
    "def model_FC(inputs):\n",
    "    \n",
    "    f1 = Flatten()(inputs)\n",
    "    f1 = Dense(output_dim=64, activation='sigmoid', init='normal')(f1)\n",
    "    f1 = Dropout(.3)(f1)\n",
    "    inc_output = Dense(output_dim=1, activation='sigmoid', init='normal')(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Nadam(lr=0.00001, beta_1=0.9, beta_2=0.999,\n",
    "                                  epsilon=1e-08, schedule_decay=0.01),\n",
    "                                  metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot a 20x20 SiPM map\n",
    "# -- carried over from NEW_kr_diff_mc_train.ipynb\n",
    "def NEW_SiPM_map_plot(xarr, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots a SiPM map in the NEW Geometry\n",
    "    xarr is a NEW sipm map, yarr the pair of coordinates the map corresponds to\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        probs = (xarr - np.min(xarr))\n",
    "        probs /= np.max(probs)\n",
    "    else: \n",
    "        probs = xarr\n",
    "\n",
    "    # set up the figure\n",
    "    fig = plt.figure();\n",
    "    ax1 = fig.add_subplot(111);\n",
    "    fig.set_figheight(5.0)\n",
    "    fig.set_figwidth(5.0)\n",
    "    ax1.axis([-100, 100, -100, 100]);\n",
    "\n",
    "    for i in range(20):\n",
    "        for j in range(20):\n",
    "            r = Ellipse(xy=(i * 10 - 95, j * 10 - 95), width=2., height=2.);\n",
    "            r.set_facecolor('0');\n",
    "            r.set_alpha(probs[i, j]);\n",
    "            ax1.add_artist(r);\n",
    "        \n",
    "    plt.xlabel(\"x (mm)\");\n",
    "    plt.ylabel(\"y (mm)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "x_train, y_train = read_data(data_location, run_name, train_fstart, train_fend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot one slice of one event\n",
    "NEW_SiPM_map_plot(x_train[0,:,:,18,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define and train the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set load_model to true and specify the file to load in a previously defined/trained model\n",
    "load_model = False\n",
    "mfile = 'models/conv3d_classifier.h5'\n",
    "\n",
    "if(load_model):\n",
    "    model = load_model(mfile)\n",
    "else:\n",
    "    \n",
    "    # otherwise define the model\n",
    "    inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "    model = model_FC(inputs)\n",
    "    \n",
    "    # define callbacks (actions to be taken after each epoch of training)\n",
    "    file_lbl = \"{epoch:02d}-{loss:.4f}\"\n",
    "    filepath=\"weights-{0}.h5\".format(file_lbl)\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    lcallbacks = [checkpoint]            \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "hist = model.fit(x_train, y_train, shuffle=True, nb_epoch=60, batch_size=100, verbose=1, validation_split=0.05, callbacks=lcallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in the test data\n",
    "x_test, y_test = read_data(data_location, run_name, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "loss_and_metrics = model.evaluate(x_test, y_test);\n",
    "y_pred = model.predict(x_test, batch_size=100, verbose=0)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
